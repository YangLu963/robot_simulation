# ğŸ¤– MultiModal HRL for Robotic Task Planning

A robotic task planning system based on **Multimodal Fusion** and **Hierarchical Reinforcement Learning (HRL)**. It parses natural language instructions into structured task plans and executes them through reinforcement learning in a simulation environment.

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ“– Table of Contents
- [âœ¨ Core Features](#-core-features)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸš€ Quick Start](#-quick-start)
- [âš™ï¸ Configuration](#ï¸-configuration)
- [ğŸ§© Core Modules](#-core-modules)
- [ğŸ“Š Data Format](#-data-format)
- [ğŸ§ª Testing & Evaluation](#-testing--evaluation)
- [ğŸ“ Citation](#-citation)
- [ğŸ¤ Contributing](#-contributing)
- [ğŸ“„ License](#-license)
- [ğŸ†˜ Support](#-support)

## âœ¨ Core Features

- **Natural Language Understanding**: Leverages Large Language Models (GPT-4) to parse user instructions into structured `RobotInstruction`.
- **Multimodal Perception**: Fuses visual (ViT) and textual (BERT) features to generate environmental state representations.
- **Hierarchical Control**: High-level (LLM) for task planning, low-level (SAC) for action execution.
- **Simulation Environment**: Integrated with PyBullet physics engine for realistic robot simulation.
- **Strong Typing Validation**: Uses Pydantic models to ensure structured and valid task instructions.

## ğŸ“ Project Structure
project_root/
â”œâ”€â”€ config/
â”‚ â”œâ”€â”€ llm_config.yaml # LLM configuration (API keys, model parameters)
â”‚ â”œâ”€â”€ rl_config.yaml # Reinforcement Learning config (SAC hyperparameters, environment spaces)
â”‚ â””â”€â”€ fusion_config.yaml # Multimodal fusion network configuration
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ controllers/ # Control layer
â”‚ â”‚ â”œâ”€â”€ hrl_controller.py # Hierarchical RL Controller
â”‚ â”‚ â””â”€â”€ llm_planner.py # LLM Task Planner
â”‚ â”œâ”€â”€ multimodal/ # Multimodal fusion module
â”‚ â”‚ â”œâ”€â”€ multimodal_model.py # Multimodal model (main entry)
â”‚ â”‚ â”œâ”€â”€ vision_encoder.py # Vision Encoder (ViT)
â”‚ â”‚ â”œâ”€â”€ text_encoder.py # Text Encoder (BERT)
â”‚ â”‚ â””â”€â”€ fusion_network.py # Cross-modal attention fusion network
â”‚ â”œâ”€â”€ reinforcement_learning/ # Reinforcement Learning module
â”‚ â”‚ â”œâ”€â”€ sac.py # Soft Actor-Critic algorithm
â”‚ â”‚ â”œâ”€â”€ environment.py # PyBullet simulation environment
â”‚ â”‚ â””â”€â”€ train.py # Training script
â”‚ â”œâ”€â”€ data/ # Data utilities
â”‚ â”‚ â”œâ”€â”€ robot_instruction.py# Task instruction Pydantic model (Core)
â”‚ â”‚ â”œâ”€â”€ data_loader.py # Data loader
â”‚ â”‚ â””â”€â”€ preprocess.py # Data preprocessing utilities
â”‚ â””â”€â”€ visualization/ # Visualization tools
â”‚ â””â”€â”€ plot_utils.py # Plotting utility functions
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ download_models.sh # Download pre-trained models
â”‚ â”œâ”€â”€ evaluate.py # Model evaluation script
â”‚ â””â”€â”€ deploy.py # Model deployment script (to ONNX)
â”œâ”€â”€ models/
â”‚ â””â”€â”€ llm_planner/
â”‚ â””â”€â”€ task_templates.py # LLM task templates
â””â”€â”€ multimodal_fusion.py
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ LICENSE # MIT License
â””â”€â”€ README.md # Project description
â””â”€â”€ .DS_Store


text

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
# Clone the repository
git clone https://github.com/YangLu963/work.git
cd work

# Create a conda environment (recommended)
conda create -n robot-hrl python=3.8
conda activate robot-hrl

# Install core dependencies
pip install torch torchvision torchaudio
pip install transformers einops pydantic pybullet gymnasium
pip install openai python-dotenv

# Alternatively, use requirements.txt
pip install -r requirements.txt
2. Download Pre-trained Models
bash
# Run the download script
bash scripts/download_models.sh
3. Configure API Keys
bash
# Copy the environment variables template
cp .env.example .env
# Edit the .env file and add your OpenAI API key
OPENAI_API_KEY=sk-your-api-key-here
4. Run a Demo
bash
# Test the LLM planner (Parse natural language instruction)
python -c "
from src.controllers.llm_planner import LLMController
controller = LLMController()
plan = controller.plan_task('Put the red block into the blue box')
print(plan.json(indent=2))
"

# Run the main training program
python src/main.py
âš™ï¸ Configuration
LLM Configuration (config/llm_config.yaml)
yaml
llm:
  api_key: ""  # Your OpenAI API Key
  model: "gpt-4-1106-preview"
  temperature: 0.7
  max_tokens: 1024
  task_templates_path: "models/llm_planner/task_templates.json"
Reinforcement Learning Configuration (config/rl_config.yaml)
yaml
rl:
  policy:
    type: "sac"
    gamma: 0.99
    tau: 0.005
    lr: 3e-4
  obs_space:
    rgb_shape: [224, 224, 3]
    joint_dim: 7
  action_space:
    dim: 7
    low: [-1.0, -1.0, -1.0, -3.14, -3.14, -3.14, -0.5]
    high: [1.0, 1.0, 1.0, 3.14, 3.14, 3.14, 0.5]
ğŸ§© Core Modules
Task Instruction Structure
python
# Strongly-typed instruction model based on Pydantic
class RobotInstruction(BaseModel):
    intent: Literal["transfer", "arrange", "clean", "fetch"]
    objects: List[Object]
    steps: List[Step]
    safety_constraints: List[SafetyConstraint] = []
Multimodal Fusion Network
python
# Fuses visual and textual features
class MultimodalFusionNetwork(nn.Module):
    def __init__(self, visual_feat_dim=768, text_feat_dim=768, hidden_dim=512):
        super().__init__()
        self.visual_proj = nn.Linear(visual_feat_dim, hidden_dim)
        self.text_proj = nn.Linear(text_feat_dim, hidden_dim)
        self.cross_attn = nn.MultiheadAttention(hidden_dim, num_heads=8)
ğŸ“Š Data Format
Task instruction data is in JSON format, compliant with the Pydantic model definition:

json
{
  "intent": "transfer",
  "objects": [
    {"id": 1, "name": "cup", "attributes": {"color": "red"}}
  ],
  "steps": [
    {
      "action": "pick",
      "target": {"object_id": 1},
      "preconditions": [{"type": "object_visible", "object_id": 1}]
    }
  ]
}
ğŸ§ª Testing & Evaluation
bash
# Run the evaluation script
python scripts/evaluate.py --model_path models/multimodal_fusion.pt

# Generate training curves
python -c "
from src.visualization.plot_utils import plot_training_history
plot_training_history('outputs/training_log.txt')
"
ğŸ“ Citation
If you use this project in your research, please cite:

bibtex
@software{yang2025multimodalhrl,
  title = {MultiModal HRL for Robotic Task Planning},
  author = {Yang Lu},
  year = {2025},
  url = {https://github.com/YangLu963/work}
}
ğŸ¤ Contributing
Contributions are welcome! Please feel free to submit Issues and Pull Requests. Ensure you:

Follow the existing code style.

Add appropriate test cases.

Update relevant documentation.

ğŸ“„ License
This project is licensed under the MIT License - see the LICENSE file for details.

ğŸ†˜ Support
If you encounter any problems:

ğŸ“§ Email: luyang96377@gmail.com

ğŸ› Issues: GitHub Issues

ğŸ’¬ Discussion: Feel free to open a discussion topic

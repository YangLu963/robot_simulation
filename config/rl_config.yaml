rl:
  policy:
    type: "sac"
    gamma: 0.99
    tau: 0.005
    alpha: 0.2
    lr: 3e-4
    batch_size: 256
    hidden_dim: 512
    buffer_size: 100000
    train_interval: 4
    
  obs_space:
    # 改为与实际环境一致
    state_dim: 7  # 只有关节状态
    
  action_space:
    dim: 7
    low: [-1.0, -1.0, -1.0, -3.14, -3.14, -3.14, -0.5]
    high: [1.0, 1.0, 1.0, 3.14, 3.14, 3.14, 0.5]
    clip: true
    
  env:
    max_episode_steps: 500
    reward_scale: 1.0
    success_threshold: 0.95
